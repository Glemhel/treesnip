---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
library(magrittr)
library(magrittr)
library(knitr)
library(kableExtra)
```

# treesnip

<!-- badges: start -->
[![R build status](https://github.com/curso-r/treesnip/workflows/R-CMD-check/badge.svg)](https://github.com/curso-r/treesnip)
<!-- badges: end -->

This package provides the following bindings for parsnip package:

- the `tree` engine for `decision_tree`;
- the `catboost` engine for `boost_tree`;
- the `lightGBM` engine for `boost_tree`.

**docs**

- [tree package docs](https://cran.r-project.org/web/packages/tree/tree.pdf)
- [LightGBM docs](https://lightgbm.readthedocs.io/)
- [Catboost docs](https://catboost.ai/docs/)

## Installation

You can install the released version of treesnip from [CRAN](https://CRAN.R-project.org) with:

``` r
remotes::install_github("curso-r/treesnip")
```

## Roadmap

```{r, echo=FALSE, warning=FALSE, message=FALSE}
ok <- function(x = "") {
  if(x == "x") ":heavy_check_mark:"
  else if(x == "n") ":white_circle:"
  else ":red_circle:"
}
tibble::tribble(
  ~fun,              ~tree,    ~catboost,    ~lightGBM,
  "set_fit",       ok("x"),      ok("x"),      ok(),
  "set_model_arg", ok("x"),      ok(),         ok(),
  "set_pred",      ok("x"),      ok("x"),      ok(),
  "train",         ok("x"),      ok("x"),      ok(),
  "predict",       ok("x"),      ok("x"),      ok(),
  "multi_predict", ok("n"),      ok("x"),      ok(),
  "tests",         ok("x"),      ok(),         ok()
) %>% kable() 
```


## Hyperparameters map

**decision_tree()**

```{r, echo=FALSE}
tibble::tribble(
  ~ parsnip, ~tree, 
  "min_n", "minsize",
  "cost_complexity", "mindev"
) %>% knitr::kable()
```

**boost_tree()**

```{r, echo=FALSE, warning=FALSE, message=FALSE}
tibble::tribble(
  ~ parsnip, ~catboost, ~lightGBM,
  'mtry', 'rsm', 'feature_fraction',
  'trees', 'iterations', 'num_iterations',
  'min_n', 'min_data_in_leaf', 'min_data_in_leaf',
  'tree_depth', 'depth', 'max_depth',
  'learn_rate', 'learning_rate', 'learning_rate',
  'loss_reduction', cell_spec('Not found', color = 'red', bold = TRUE), 'min_gain_to_split',
  'sample_size', 'subsample', 'bagging_fraction'
) %>% kable(escape = FALSE) 
```


## Example

```{r}
library(treesnip)
library(tidymodels)

set.seed(1)
df <- tibble(
  x = runif(10), 
  y = 2* x + rnorm(10, sd = 0.1)
)

mod <- decision_tree(min_n = 0, cost_complexity = 0) %>%
  set_engine("tree") %>%
  set_mode("regression") %>% 
  fit(y ~ x, df)

mod

df %>% 
  mutate(pred = predict(mod, df)$.pred) %>% 
  ggplot(aes(x = x)) +
  geom_point(aes( y = y)) +
  geom_step(aes(y = pred))
```




